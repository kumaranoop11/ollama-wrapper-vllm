# ollama-wrapper-vllm
Using an OpenAI-compatible API wrapper for local Ollama deployments allows seamless integration with applications designed for vLLM deployed in the cloud
